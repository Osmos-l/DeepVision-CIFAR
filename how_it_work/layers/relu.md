# ReLU ‚Äì Rectified Linear Unit

La fonction **ReLU** est l'une des fonctions d'activation les plus utilis√©es dans les r√©seaux de neurones modernes, en particulier dans les **CNN**.

![alt text](images/relu.png)

## D√©finition

La fonction ReLU est d√©finie par :

```
ReLU(x) = max(0, x)
```

Elle remplace toutes les valeurs n√©gatives par z√©ro et laisse les positives inchang√©es.

## Illustration

```
Entr√©e : [-3, -1, 0, 2, 5]
Sortie : [ 0, 0, 0, 2, 5]
```


## Pourquoi ReLU ?

- ‚úÖ **Simplicit√©** : rapide √† calculer (pas d'exponentielle, pas de division)
- ‚úÖ **Sparsit√©** : introduit de la non-lin√©arit√© en d√©sactivant certains neurones (valeurs √† z√©ro)
- ‚úÖ **Att√©nue le probl√®me du gradient vanish** : contrairement √† la sigmo√Øde ou tanh, ReLU garde un gradient constant pour les valeurs positives

---

## O√π elle intervient dans un CNN ?

G√©n√©ralement **apr√®s chaque couche convolutionnelle ou pleinement connect√©e**, pour introduire de la non-lin√©arit√© et permettre au mod√®le d‚Äôapprendre des fonctions complexes.

---

## Ressources

- üìò *Deep Learning* ‚Äì Ian Goodfellow et al. 
